= Usage

If you are not installing the operator using Helm then after installation the CRD for this operator must be created:

    kubectl apply -f deploy/nificluster.yaml

To create a three-node Apache NiFi (v1.15.0) cluster with SingleUser authentication enabled apply the following to your Kubernetes cluster.

The admin credentials that you can then log in with are: `admin:supersecretpassword`

If you do not provide the admin user credentials the operator can auto generate a random password for you, if you enable this functionality.
You can retrieve this password by running the following command:

    kubectl get secret nifi-admin-credentials-simple -o jsonpath="{.data.password}" | base64 --decode | cat - <(echo)

You may need to adjust this command if you change the configuration for `adminCredentialsSecret` in the example below.

[source,yaml]
----
apiVersion: nifi.stackable.tech/v1alpha1
kind: NifiCluster
metadata:
  name: simple-nifi
spec:
  version: "1.15.0"
  zookeeperConfigMapName: simple-nifi-znode
  authenticationConfig:
    method:
      SingleUser:
        adminCredentialsSecret:
          name: nifi-admin-credentials-simple
          namespace: default
        aut
    allowAnonymousAccess: true
  nodes:
    roleGroups:
      default:
        selector:
          matchLabels:
            kubernetes.io/os: linux
        config:
          sensitivePropertyKeySecret: nifi-sensitive-property-key
          log:
            rootLogLevel: INFO
        replicas: 3
----

If you want to set a password for the initial admin user you can do this by applying the following object:

[source,yaml]
----
apiVersion: v1
kind: Secret
metadata:
  name: nifi-admin-credentials-simple
stringData:
  username: admin
  password: supersecretpassword
----

You can create the ZNode config map referenced in `zookeeperConfigMapName` via (assuming there is a ZooKeeper cluster called `simple-zk`:
[source,yaml]
----
apiVersion: zookeeper.stackable.tech/v1alpha1
kind: ZookeeperZnode
metadata:
  name: simple-nifi-znode
spec:
  clusterRef:
    name: simple-zk
----

== Monitoring

The managed NiFi instances are automatically configured to export Prometheus metrics. See
xref:home:operators:monitoring.adoc[] for more details.

== Configuration & Environment Overrides

The cluster definition also supports overriding configuration properties and environment variables, either per role or per role group, where the more specific override (role group) has precedence over the less specific one (role).

IMPORTANT: Do not override port numbers. This will lead to cluster malfunction.

=== Configuration Properties

Currently, not supported for any file.

=== Environment Variables

Environment variables can be (over)written by adding the `envOverrides` property.

For example per role group:

[source,yaml]
----
nodes:
  roleGroups:
    default:
      config: {}
      replicas: 1
      envOverrides:
        MY_ENV_VAR: "MY_VALUE"
----

or per role:

[source,yaml]
----
nodes:
  envOverrides:
    MY_ENV_VAR: "MY_VALUE"
  roleGroups:
    default:
      config: {}
      replicas: 1
----

=== Volume storage

By default, a Nifi cluster will create five different persistent volume claims for flow files, provenance, database, content and state folders. These PVCs will request `2Gi`. It is recommended that you configure these volume requests according to your needs.

Storage requests can be configured at role or group level, for one or more of the persistent volumes as follows:

[source,yaml]
----
nodes:
  roleGroups:
    default:
      config:
        resources:
          storage:
            flowfile_repo:
              capacity: 12Gi
            provenance_repo:
              capacity: 12Gi
            database_repo:
              capacity: 12Gi
            content_repo:
              capacity: 12Gi
            state_repo:
              capacity: 12Gi
----

In the above example, all nodes in the default group will request `12Gi` of storage the various folders.

=== Memory requests

You can request a certain amount of memory for each individual role group as shown below:

[source,yaml]
----
nodes:
  roleGroups:
    default:
      config:
        resources:
          memory:
            limit: '2Gi'
----

In this example, each node container in the "default" group will have a maximum of `2Gi` of memory.

Setting this property will automatically also set the maximum Java heap size for the corresponding process to 80% of the available memory. Be aware that if the memory constraint is too low, the cluster might fail to start. If pods terminate with an 'OOMKilled' status and the cluster doesn't start, try increasing the memory limit.

For more details regarding Kubernetes memory requests and limits see: https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/[Assign Memory Resources to Containers and Pods].

=== CPU requests

Similarly to memory resources, you can also configure CPU limits, as shown below:

[source,yaml]
----
nodes:
  roleGroups:
    default:
      config:
        resources:
          cpu:
            max: '500m'
            min: '250m'
----

For more details regarding Kubernetes CPU limits see: https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/[Assign CPU Resources to Containers and Pods].

